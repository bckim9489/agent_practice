{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ERCG8LefaX33",
        "ehIezWEaajKR",
        "lFACiazxqBeK",
        "rDXdgSpop1qb",
        "7scQ69xTbS3O",
        "kP16oCAwawq2",
        "QtEy1_mMrxoX",
        "43pV-jAmsAZf",
        "sksFSADgsLM3",
        "alqHd1KutqEv",
        "V7rLljfvxLfh",
        "Bj2GTjr4xTCv",
        "BV3CJLc87yf1",
        "InnjMGqM-anr",
        "6YZ6Asa8-gQ2",
        "irEIUmY5-p7q"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNWhTe5I1ZpTwT7Ydo70ypK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bckim9489/agent_practice/blob/main/agent_pattern.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Common Setup\n",
        "\n",
        "참고 : 아래 Secret으로 Key 등록해서 실습함\n",
        "1.   LLM API Key (GPT, claude 등) : OPENAI-KEY 로 등록했음\n",
        "2.   Tavily Serach API Key (https://app.tavily.com/home) : TAVIL-KEY 로 등록했음\n",
        "\n"
      ],
      "metadata": {
        "id": "ERCG8LefaX33"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "xaaHPNB-JuRU"
      },
      "outputs": [],
      "source": [
        "!pip -q install -U langgraph langchain-openai langchain-community wikipedia numexpr tavily-python httpx==0.27.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "# LLM API Key\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI-KEY\")\n",
        "\n",
        "# Tools\n",
        "# Search API Key\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get(\"TAVILY-KEY\")\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n"
      ],
      "metadata": {
        "id": "0of3EhdAK8Wt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ㄴ 0.1 Tools"
      ],
      "metadata": {
        "id": "rHIPs8XTcfpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.utilities import WikipediaAPIWrapper\n",
        "from langchain_community.tools import WikipediaQueryRun\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "from langchain_core.tools import tool\n",
        "import numexpr\n",
        "\n",
        "# Wikipedia tool\n",
        "wiki = WikipediaQueryRun(\n",
        "    api_wrapper=WikipediaAPIWrapper(top_k_results=3, doc_content_chars_max=2000)\n",
        ")\n",
        "\n",
        "# 기존 wiki wrapper 재사용\n",
        "wiki_lookup_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=4000)\n",
        "\n",
        "@tool\n",
        "def docstore_lookup(title: str) -> str:\n",
        "    \"\"\"\n",
        "    Lookup a specific document by title from the docstore (Wikipedia).\n",
        "    Input should be a precise title or entity name.\n",
        "    \"\"\"\n",
        "    return wiki_lookup_wrapper.run(title)\n",
        "\n",
        "# Tavily Search tool\n",
        "search_tool = TavilySearchResults(max_results=5)\n",
        "\n",
        "# Calculator tool (llm-math 대체)\n",
        "@tool\n",
        "def calculator(expression: str) -> str:\n",
        "    \"\"\"\n",
        "    Evaluate a mathematical expression.\n",
        "    Input must be a valid expression like '345*872' or '12/4+9'.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return str(numexpr.evaluate(expression))\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "\n",
        "tools = [wiki, calculator, search_tool, docstore_lookup]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyVc3d97jxUh",
        "outputId": "b6436235-7515-4656-dfa9-b839cdd11c38"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4054356365.py:25: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the `langchain-tavily package and should be used instead. To use it run `pip install -U `langchain-tavily` and import as `from `langchain_tavily import TavilySearch``.\n",
            "  search_tool = TavilySearchResults(max_results=5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Zero-Shot ReAct\n",
        "*   기억 없음(Stateless)\n",
        "*   **LLM + Tools**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ehIezWEaajKR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ㄴ 1.1 Description"
      ],
      "metadata": {
        "id": "YAnx3hYxpLVg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ㄴ 1.1.1 Workflow\n"
      ],
      "metadata": {
        "id": "hYUi-RFIpe9p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```javascript\n",
        "Client\n",
        "  ↓\n",
        "User Question 전송\n",
        "  ↓\n",
        "API / Backend (Agent Endpoint)\n",
        "  ↓\n",
        "LangGraph Runtime 시작\n",
        "  ↓\n",
        "LLM: 질문 해석 (Thought / Reasoning)\n",
        "  ↓\n",
        "Tool 필요 여부 판단\n",
        "  ├─ 필요 없음 → 바로 답 생성\n",
        "  └─ 필요 있음 → Tool 선택 (Action)\n",
        "                      ↓\n",
        "                Tool 실행 (Python / API / DB 등)\n",
        "                      ↓\n",
        "                결과 반환 (Observation)\n",
        "                      ↓\n",
        "LLM이 결과 반영하여 다시 Reasoning\n",
        "  ↓\n",
        "(필요 시 Thought → Action → Observation 반복)\n",
        "  ↓\n",
        "Final Answer 생성\n",
        "  ↓\n",
        "API가 응답 반환\n",
        "  ↓\n",
        "Client가 사용자에게 표시\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "9uI9XCOlqjvb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ㄴ 1.1.2 Inner ReAct Loop"
      ],
      "metadata": {
        "id": "lFACiazxqBeK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "``` javascript\n",
        "while 문제 해결 전:\n",
        "    Thought      → 어떻게 풀지 스스로 판단\n",
        "    Action       → 사용할 Tool 선택\n",
        "    Observation  → Tool 실행 결과 받기\n",
        "    Thought      → 결과 보고 다음 행동 결정\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "gQYu7VPTqTip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ㄴ 1.2 Code"
      ],
      "metadata": {
        "id": "rDXdgSpop1qb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "agent = create_react_agent(llm, tools)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mcT3ELCjyDZ",
        "outputId": "534357e9-118d-4f5f-ed65-04d6dd388ebd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2357398768.py:3: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
            "  agent = create_react_agent(llm, tools)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q = \"\"\"위키피디아에서 찾기 좋은 '검색어(제목형 키워드)'를 3개 만들어서,\n",
        "그 키워드로 Wikipedia tool을 사용해 검색하고 근거를 요약해줘.\n",
        "주제: Pinus densiflora 옮겨심기(이식) 적기\"\"\""
      ],
      "metadata": {
        "id": "vSI9contrFxX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = agent.invoke({\"messages\": [(\"user\", q)]})\n",
        "\n",
        "print(result[\"messages\"][-1].content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "tHHfmPKBrDtq",
        "outputId": "82948ee9-8ab9-4294-c75b-730a8b5cf810"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "위키피디아에서 \"Pinus densiflora transplanting season\", \"best time to transplant Japanese red pine\", \"Pinus densiflora cultivation and care\"라는 검색어로 검색을 시도했으나, 관련된 유의미한 결과를 찾을 수 없었습니다. \n",
            "\n",
            "이 주제에 대한 정보를 얻기 위해서는 다른 방법을 사용하거나, 보다 구체적인 자료를 찾아보는 것이 필요할 수 있습니다. 예를 들어, 원예 관련 서적이나 전문 웹사이트에서 정보를 찾는 것이 도움이 될 수 있습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ㄴ 1.3 Verbose"
      ],
      "metadata": {
        "id": "7scQ69xTbS3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage, ToolMessage\n",
        "\n",
        "for step in agent.stream(\n",
        "    {\"messages\": [(\"user\", q)]},\n",
        "    stream_mode=\"values\",\n",
        "):\n",
        "    msg = step[\"messages\"][-1]\n",
        "\n",
        "    # LLM이 생각/결정한 내용\n",
        "    if isinstance(msg, AIMessage):\n",
        "        print(\"\\n AI MESSAGE\")\n",
        "        print(msg.content)\n",
        "\n",
        "        if getattr(msg, \"tool_calls\", None):\n",
        "            print(\" TOOL CALL:\", msg.tool_calls)\n",
        "\n",
        "    # Tool 실행 결과\n",
        "    elif isinstance(msg, ToolMessage):\n",
        "        print(\"\\n TOOL RESULT\")\n",
        "        print(\"tool:\", msg.name)\n",
        "        print(msg.content[:1000])  # 너무 길면 앞부분만\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mWt9u_sNjzzv",
        "outputId": "a7f40c96-aab6-447d-98d2-680cf5d9b2c3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " AI MESSAGE\n",
            "\n",
            " TOOL CALL: [{'name': 'wikipedia', 'args': {'query': 'Pinus densiflora transplanting season'}, 'id': 'call_nA2Nz5ZCaBsJ690QlYhHQnrH', 'type': 'tool_call'}, {'name': 'wikipedia', 'args': {'query': 'best time to transplant Japanese red pine'}, 'id': 'call_JrITnlpgs5AYptyvmBRdvTjz', 'type': 'tool_call'}, {'name': 'wikipedia', 'args': {'query': 'Pinus densiflora cultivation and care'}, 'id': 'call_SGoTvyZyVMRqo6HKN4kFu6UZ', 'type': 'tool_call'}]\n",
            "\n",
            " TOOL RESULT\n",
            "tool: wikipedia\n",
            "No good Wikipedia Search Result was found\n",
            "\n",
            " AI MESSAGE\n",
            "위키피디아에서 \"Pinus densiflora transplanting season\", \"best time to transplant Japanese red pine\", \"Pinus densiflora cultivation and care\"라는 검색어로 검색을 시도했으나, 관련된 유의미한 결과를 찾을 수 없었습니다. \n",
            "\n",
            "이 주제에 대한 정보를 얻기 위해서는 다른 방법을 사용하거나, 보다 구체적인 자료를 찾아보는 것이 필요할 수 있습니다. 예를 들어, 원예 관련 서적이나 전문 웹사이트에서 정보를 찾는 것이 도움이 될 수 있습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BrN_rPDn1ohp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Conversational ReAct\n",
        "*   메모리 사용\n",
        "*   **LLM + Tools + Checkpointer**\n",
        "\n"
      ],
      "metadata": {
        "id": "kP16oCAwawq2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ㄴ 2.1 Description"
      ],
      "metadata": {
        "id": "QtEy1_mMrxoX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ㄴ 2.1.1 Workflow"
      ],
      "metadata": {
        "id": "43pV-jAmsAZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```javascript\n",
        "Client\n",
        "  ↓\n",
        "User Question 전송\n",
        "  ↓\n",
        "API / Backend (Agent Endpoint)\n",
        "  ↓\n",
        "LangGraph Runtime 시작\n",
        "  ↓\n",
        "이전 대화 기록(Memory / State) 로드\n",
        "  ↓\n",
        "LLM: 현재 질문 + 대화 히스토리 함께 해석\n",
        "  ↓\n",
        "사용자의 의도 파악 (맥락 기반 Reasoning)\n",
        "  ↓\n",
        "Tool 필요 여부 판단\n",
        "  ├─ 필요 없음 → 바로 답 생성\n",
        "  └─ 필요 있음 → Tool 선택 (Action)\n",
        "                      ↓\n",
        "                Tool 실행 (Python / API / DB 등)\n",
        "                      ↓\n",
        "                결과 반환 (Observation)\n",
        "                      ↓\n",
        "LLM이 결과 + 이전 대화 맥락을 반영하여 다시 Reasoning\n",
        "  ↓\n",
        "(필요 시 Thought → Action → Observation 반복)\n",
        "  ↓\n",
        "Final Answer 생성\n",
        "  ↓\n",
        "대화 Memory(State)에 이번 메시지 저장\n",
        "  ↓\n",
        "API가 응답 반환\n",
        "  ↓\n",
        "Client가 사용자에게 표시\n",
        "```"
      ],
      "metadata": {
        "id": "0E4iTxHisGtf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ㄴ 2.1.2 Inner ReAct Loop"
      ],
      "metadata": {
        "id": "sksFSADgsLM3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```javascript\n",
        "while 문제 해결 전:\n",
        "    Load Memory        → 이전 대화 불러오기\n",
        "    Thought            → 맥락 기반으로 판단\n",
        "    Action             → Tool 필요하면 실행\n",
        "    Observation        → Tool 결과 받기\n",
        "    Update Memory      → 새 정보 저장\n",
        "```"
      ],
      "metadata": {
        "id": "uvgeHLPnsOn2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ㄴ 2.2 Code"
      ],
      "metadata": {
        "id": "BZJtKmFur3ly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.prebuilt import create_react_agent\n",
        "from langgraph.checkpoint.memory import InMemorySaver\n",
        "\n",
        "checkpointer = InMemorySaver()\n",
        "agent = create_react_agent(llm, tools, checkpointer=checkpointer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5j7GyaUZjqv",
        "outputId": "37ab46f9-2882-4da2-f142-7d6880ec638d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1647517424.py:5: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
            "  agent = create_react_agent(llm, tools, checkpointer=checkpointer)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"chat-001\"}}\n",
        "q1 = \"내 출생년도는 1994년이야. 지금은 계산하지 말고 이 정보만 기억해.\"\n",
        "q2 = \"그럼 내가 2026년에 몇 살인지 계산기를 꼭 사용해서 나이를 계산해서 알려줘\"\n",
        "q3 = \"올해는 무슨 해이며 나는 무슨 띠인지 알려줘\""
      ],
      "metadata": {
        "id": "gMf3OMaGZ7ku"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = agent.invoke({\"messages\": [(\"user\", q1)]}, config=config)\n",
        "print(result[\"messages\"][-1].content)\n",
        "result = agent.invoke({\"messages\": [(\"user\", q2)]}, config=config)\n",
        "print(result[\"messages\"][-1].content)\n",
        "result = agent.invoke({\"messages\": [(\"user\", q3)]}, config=config)\n",
        "print(result[\"messages\"][-1].content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUUVKU_wZpaI",
        "outputId": "0caaffd9-1dad-4edd-dbf7-86404b64c95e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "알겠습니다! 당신의 출생년도는 1994년입니다. 이 정보를 기억하겠습니다.\n",
            "2026년에 당신은 32살이 됩니다.\n",
            "2023년은 \"계묘년\"으로, 토끼의 해입니다. 그리고 당신이 태어난 1994년은 \"갑술년\"으로, 개의 띠입니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ㄴ 2.3 Verbose"
      ],
      "metadata": {
        "id": "fVi1sXZ-bgqf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ㄴ 2.3.1 Test Setup"
      ],
      "metadata": {
        "id": "zA7HHzH8kuhi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage, ToolMessage\n",
        "import time\n",
        "from openai import RateLimitError\n",
        "\n",
        "from langchain_core.messages import AIMessage, ToolMessage\n",
        "\n",
        "def stream_with_retry(agent, payload, config, stream_mode=\"values\", max_retries=6):\n",
        "    \"\"\"\n",
        "    agent.stream(...) 를 RateLimitError(429) 발생 시 backoff 재시도.\n",
        "    \"\"\"\n",
        "    attempt = 0\n",
        "    while True:\n",
        "        try:\n",
        "            for step in agent.stream(payload, config=config, stream_mode=stream_mode):\n",
        "                yield step\n",
        "            return  # 정상 종료\n",
        "        except RateLimitError as e:\n",
        "            attempt += 1\n",
        "            if attempt > max_retries:\n",
        "                raise\n",
        "\n",
        "            # 간단 backoff (점점 더 기다림)\n",
        "            wait = min(20, 1.5 * attempt)\n",
        "            print(f\"\\n RateLimit(429). retry in {wait:.1f}s ...\")\n",
        "            time.sleep(wait)\n",
        "\n",
        "\n",
        "\n",
        "def run_verbose(question: str, label: str, thread_id: str):\n",
        "    print(\"\\n\" + \"=\"*20 + f\" {label} (thread_id={thread_id}) \" + \"=\"*20)\n",
        "    print(\"USER:\", question)\n",
        "\n",
        "    cfg = {\"configurable\": {\"thread_id\": thread_id}}\n",
        "\n",
        "    payload = {\"messages\": [(\"user\", question)]}\n",
        "\n",
        "    last_ai = None\n",
        "    for step in stream_with_retry(agent, payload, cfg, stream_mode=\"values\"):\n",
        "        msg = step[\"messages\"][-1]\n",
        "\n",
        "        if isinstance(msg, AIMessage):\n",
        "            last_ai = msg\n",
        "            if msg.content:\n",
        "                print(\"\\nAI:\", msg.content)\n",
        "            if getattr(msg, \"tool_calls\", None):\n",
        "                print(\"tool_calls:\", msg.tool_calls)\n",
        "\n",
        "        elif isinstance(msg, ToolMessage):\n",
        "            print(\"\\nTOOL RESULT:\", msg.name)\n",
        "            print(msg.content[:800])\n",
        "\n",
        "    return last_ai.content if last_ai else None\n",
        "\n",
        "#메모리 사용하는지 A/B 테스트\n",
        "def ab_test(q1: str, q2: str, q3:str, thread_a: str=\"mem-A\", thread_b: str=\"mem-B\", thread_c: str=\"mem-C\"):\n",
        "    print(\"\\n\\n\" + \"#\"*10 + \" A/B MEMORY TEST START \" + \"#\"*10)\n",
        "\n",
        "    # A: q1 using thread_id a\n",
        "    run_verbose(q1, \"A-1: seed memory (q1)\", thread_a)\n",
        "\n",
        "\n",
        "    # B: q2 using thread_id b\n",
        "    run_verbose(q2, \"B-1: q2 only (no memory)\", thread_b)\n",
        "\n",
        "\n",
        "    # B: q3 using thread_id c\n",
        "    run_verbose(q3, \"C-1: q3 only (no memory)\", thread_c)\n",
        "\n",
        "    print(\"\\n\" + \"#\"*10 + \" A/B MEMORY TEST END \" + \"#\"*10)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2LB9H_IbaSK1"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ㄴ 2.3.2 Test A (같은 thread_id)\n",
        "\n",
        "*   Q1을 thread_id A 에 저장\n",
        "*   Q2를 thread_id A 에 질의\n",
        "----------------------------\n",
        "| Q2에서 메모리에 저장한 Q1을 활용하여 답변할 것으로 예상"
      ],
      "metadata": {
        "id": "PN7owphAlH3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 같은 thread_id\n",
        "run_verbose(q1, \"Q1\", \"chat-001\")\n",
        "run_verbose(q2, \"Q2\", \"chat-001\")\n",
        "run_verbose(q3, \"Q3\", \"chat-001\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        },
        "collapsed": true,
        "id": "nwQbCVIOlA3x",
        "outputId": "06a0a199-ccb2-4abf-ec69-cd116cdd23e4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================== Q1 (thread_id=chat-001) ====================\n",
            "USER: 내 출생년도는 1994년이야. 지금은 계산하지 말고 이 정보만 기억해.\n",
            "\n",
            "AI: 알겠습니다! 당신의 출생년도는 1994년입니다. 이 정보를 기억하겠습니다.\n",
            "\n",
            "==================== Q2 (thread_id=chat-001) ====================\n",
            "USER: 그럼 내가 2026년에 몇 살인지 계산기를 꼭 사용해서 나이를 계산해서 알려줘\n",
            "tool_calls: [{'name': 'calculator', 'args': {'expression': '2026-1994'}, 'id': 'call_EjyEA8vW8d8tGm6zRh40VJEy', 'type': 'tool_call'}]\n",
            "\n",
            "TOOL RESULT: calculator\n",
            "32\n",
            "\n",
            "AI: 2026년에 당신은 32살이 됩니다.\n",
            "\n",
            "==================== Q3 (thread_id=chat-001) ====================\n",
            "USER: 올해는 무슨 해이며 나는 무슨 띠인지 알려줘\n",
            "\n",
            " RateLimit(429). retry in 1.5s ...\n",
            "\n",
            " RateLimit(429). retry in 3.0s ...\n",
            "tool_calls: [{'name': 'tavily_search_results_json', 'args': {'query': '2023년은 무슨 해'}, 'id': 'call_mGd5WnSNbmfm9kF48YkLh7f1', 'type': 'tool_call'}, {'name': 'tavily_search_results_json', 'args': {'query': '1994년은 무슨 띠'}, 'id': 'call_Pot4oxzGOHM2SrljvgZdCoz7', 'type': 'tool_call'}]\n",
            "\n",
            "TOOL RESULT: tavily_search_results_json\n",
            "[{\"title\": \"1994 - Wikipedia\", \"url\": \"https://en.wikipedia.org/wiki/1994\", \"content\": \"| Gregorian calendar | 1994 MCMXCIV |\\n| Ab urbe condita | 2747 |\\n| Armenian calendar | 1443 ԹՎ ՌՆԽԳ |\\n| Assyrian calendar | 6744 |\\n| Baháʼí calendar | 150–151 |\\n| Balinese saka calendar | 1915–1916 |\\n| Bengali calendar | 1400–1401 |\\n| Berber calendar | 2944 |\\n| British Regnal year | 42 Eliz. 2 – 43 Eliz. 2 |\\n| Buddhist calendar | 2538 |\\n| Burmese calendar | 1356 |\\n| Byzantine calendar | 7502–7503 |\\n| Chinese calendar | 癸酉年 (Water Rooster \\\"Rooster (zodiac)\\\")) 4691 or 4484     — to — 甲戌年 (Wood Dog \\\"Dog (zodiac)\\\")) 4692 or 4485 |\\n| Coptic calendar | 1710–1711 |\\n| Discordian calendar | 3160 |\\n| Ethiopian calendar | 1986–1987 |\\n| Hebrew calendar | 5754–5755 |\\n| Hindu calendars |\\n| - Vik\n",
            "\n",
            "AI: 2023년은 \"계묘년\"으로, 토끼의 해입니다. 그리고 당신이 태어난 1994년은 \"갑술년\"으로, 개의 띠입니다.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2023년은 \"계묘년\"으로, 토끼의 해입니다. 그리고 당신이 태어난 1994년은 \"갑술년\"으로, 개의 띠입니다.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ㄴ 2.3.3 TEST B (다른 thread_id)\n",
        "\n",
        "*   Q1을 thread_id A 에 저장\n",
        "*   Q2를 thread_id B에 질의\n",
        "----------------------------\n",
        "| Q2에서 질의 에 대한 정보 요구 예상\n",
        "\n"
      ],
      "metadata": {
        "id": "y1eAeR0BlLcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ab_test(q1, q2, q3, thread_a=\"chat-003\", thread_b=\"chat-004\", thread_c=\"chat-005\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0Qi1B1S2lM3i",
        "outputId": "0f1d6be0-d8f1-4299-cdea-870eb5f5dfd4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "########## A/B MEMORY TEST START ##########\n",
            "\n",
            "==================== A-1: seed memory (q1) (thread_id=chat-003) ====================\n",
            "USER: 내 출생년도는 1994년이야. 지금은 계산하지 말고 이 정보만 기억해.\n",
            "\n",
            " RateLimit(429). retry in 1.5s ...\n",
            "\n",
            " RateLimit(429). retry in 3.0s ...\n",
            "\n",
            " RateLimit(429). retry in 4.5s ...\n",
            "\n",
            "AI: 알겠습니다. 당신의 출생년도는 1994년입니다. 이 정보를 기억하겠습니다.\n",
            "\n",
            "==================== B-1: q2 only (no memory) (thread_id=chat-004) ====================\n",
            "USER: 그럼 내가 2026년에 몇 살인지 계산기를 꼭 사용해서 나이를 계산해서 알려줘\n",
            "tool_calls: [{'name': 'calculator', 'args': {'expression': '2026-2023'}, 'id': 'call_uoBeNsrFAf1XEqiW0ynZPVcN', 'type': 'tool_call'}]\n",
            "\n",
            "TOOL RESULT: calculator\n",
            "3\n",
            "\n",
            "AI: 2026년은 현재로부터 3년 후입니다. 따라서, 현재 나이에 3을 더하면 2026년에 몇 살인지 알 수 있습니다. 현재 나이를 알려주시면 2026년에 몇 살인지 계산해드릴 수 있습니다.\n",
            "\n",
            "==================== C-1: q3 only (no memory) (thread_id=chat-005) ====================\n",
            "USER: 올해는 무슨 해이며 나는 무슨 띠인지 알려줘\n",
            "\n",
            "AI: 2023년은 음력으로 계묘년(癸卯年)이며, 이는 토끼띠에 해당합니다. 당신이 무슨 띠인지 알려면 태어난 해를 알아야 합니다. 태어난 해를 알려주시면 띠를 계산해드릴 수 있습니다.\n",
            "\n",
            "########## A/B MEMORY TEST END ##########\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Search-augmented ReAct (Self-ask with search)\n",
        "\n",
        "*   질문 분해 중심\n",
        "*   **RAG와 매우 유사**\n",
        "*   사실상 Self-Ask = 초기 형태의 RAG\n",
        "\n"
      ],
      "metadata": {
        "id": "alqHd1KutqEv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ㄴ 3.1 Description"
      ],
      "metadata": {
        "id": "ZsEO8BqCxETT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ㄴ 3.1.1. Workflow"
      ],
      "metadata": {
        "id": "V7rLljfvxLfh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```javascript\n",
        "Client\n",
        "  ↓\n",
        "User Question 전송\n",
        "  ↓\n",
        "API / Backend (Agent Endpoint)\n",
        "  ↓\n",
        "LangGraph / Agent Runtime 시작\n",
        "  ↓\n",
        "LLM이 질문을 \"하위 질문(Sub-questions)\"으로 분해\n",
        "  ↓\n",
        "각 하위 질문을 Search Tool로 순차 조회\n",
        "  ↓\n",
        "검색 결과(Observation)를 모아 중간 결론 생성\n",
        "  ↓\n",
        "필요하면 추가 하위 질문 생성 → 다시 Search\n",
        "  ↓\n",
        "모든 정보가 충분해지면 최종 답 생성\n",
        "  ↓\n",
        "API가 응답 반환\n",
        "  ↓\n",
        "Client가 사용자에게 표시\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "fy4VuWnJxPWb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ㄴ 3.1.2 Inner ReAct Loop"
      ],
      "metadata": {
        "id": "Bj2GTjr4xTCv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```javascript\n",
        "while 답을 만들 정보가 부족하면:\n",
        "    Follow-up Question 생성\n",
        "    Search Tool 실행\n",
        "    Observation 수집\n",
        "    현재까지의 사실 정리\n",
        "```"
      ],
      "metadata": {
        "id": "EoMIb-d0xYXs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ㄴ 3.2 Code"
      ],
      "metadata": {
        "id": "vuRDO56bxdTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage\n",
        "\n",
        "# Self-Ask의 핵심 포맷을 강제\n",
        "SELF_ASK_POLICY = SystemMessage(content=\"\"\"\n",
        "You are a Self-Ask-with-Search agent.\n",
        "\n",
        "Hard rules:\n",
        "- For EACH follow-up question, you MUST call the tavily_search_results_json tool.\n",
        "- After the tool returns, you MUST write an Intermediate answer that uses the tool result.\n",
        "- Intermediate answer must include 2-3 bullet points and mention the source titles (from tool results).\n",
        "- Do NOT write placeholders like \"...\". If info is missing, do another search.\n",
        "\n",
        "Output format (exact):\n",
        "Follow-up 1: ...\n",
        "Intermediate answer 1:\n",
        "- ...\n",
        "- ...\n",
        "Sources: <title1>, <title2>\n",
        "\n",
        "Follow-up 2: ...\n",
        "Intermediate answer 2:\n",
        "- ...\n",
        "- ...\n",
        "Sources: ...\n",
        "\n",
        "Final answer: ...\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "id": "aBV7rR953T53"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "agent = create_react_agent(llm, tools)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iImBDY1xhqV",
        "outputId": "bf147ce3-480c-4104-c3b5-1d9debc7aa13"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2357398768.py:3: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
            "  agent = create_react_agent(llm, tools)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q = \"\"\"AI Agent를 3문장으로 설명해줘.\n",
        "단, 2025~2026년 기준으로 최신 경향을 반영하기 위해 반드시 tavily_search_results_json을 최소 2번 사용하고,\n",
        "각 검색 결과를 근거로 Intermediate answer를 작성한 뒤 Final answer를 써.\"\"\""
      ],
      "metadata": {
        "id": "aSD-JJbi19tl"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"self-ask-01\"}}\n",
        "\n",
        "result = agent.invoke({\"messages\": [SELF_ASK_POLICY, (\"user\", q)]},config=config)\n",
        "\n",
        "print(result[\"messages\"][-1].content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JKLGdqQ11pJ",
        "outputId": "654859ab-83fb-4159-c1fe-1313db19d1bf"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intermediate answer 1:\n",
            "- In 2025, AI agents are evolving into sophisticated systems capable of complex reasoning and collaboration, moving beyond single-purpose bots to more holistic, task-oriented systems. This includes trends like Agentic RAG and multi-agent systems (MarkTechPost, [x]cube LABS).\n",
            "- The rise of multi-agent systems is significant, where teams of specialized agents work together to solve complex problems, mirroring human teamwork. This trend is reshaping enterprise AI applications, with a focus on specialized, vertical solutions (MarkTechPost, [x]cube LABS).\n",
            "Sources: MarkTechPost, [x]cube LABS\n",
            "\n",
            "Intermediate answer 2:\n",
            "- By 2026, AI agents are expected to become integral to business processes, acting more like teammates than tools. This includes the shift from individual usage to team and workflow orchestration, enhancing productivity and decision-making (Google, Microsoft Source).\n",
            "- The democratization of AI agent creation is anticipated, allowing everyday business users to design and deploy intelligent agents, thus driving innovation and solving real-world problems (IBM, Anthropic).\n",
            "Sources: Google, Microsoft Source, IBM, Anthropic\n",
            "\n",
            "Final answer:\n",
            "In 2025, AI agents are transforming into complex, reasoning-driven systems capable of collaboration, with trends like Agentic RAG and multi-agent systems leading the way. By 2026, these agents are expected to become integral to business processes, acting as teammates and enhancing productivity. The democratization of AI agent creation will empower everyday users to innovate and solve real-world problems, marking a significant shift in how AI is utilized across industries.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ㄴ 3.3. Verbose"
      ],
      "metadata": {
        "id": "LrTWqWT0xjtq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage, ToolMessage\n",
        "\n",
        "def run_self_ask_verbose(question: str, thread_id=\"self-ask-03\", tool_preview=600):\n",
        "    cfg = {\"configurable\": {\"thread_id\": thread_id}}\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"USER:\", question)\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    for step in agent.stream(\n",
        "        {\"messages\": [SELF_ASK_POLICY, (\"user\", question)]},\n",
        "        config=cfg,\n",
        "        stream_mode=\"values\",\n",
        "    ):\n",
        "        msg = step[\"messages\"][-1]\n",
        "\n",
        "        # 1) 모델이 말한 텍스트(중간 출력 포함)\n",
        "        if isinstance(msg, AIMessage):\n",
        "            if msg.content:  # content가 있는 경우만 출력\n",
        "                print(\"\\n AI MESSAGE:\\n\", msg.content)\n",
        "\n",
        "            # 2) 도구 호출 로그\n",
        "            if getattr(msg, \"tool_calls\", None):\n",
        "                print(\"\\n TOOL CALLS:\")\n",
        "                for tc in msg.tool_calls:\n",
        "                    print(\" -\", tc)\n",
        "\n",
        "        # 3) 도구 결과\n",
        "        elif isinstance(msg, ToolMessage):\n",
        "            print(\"\\n TOOL RESULT:\", msg.name)\n",
        "            print(msg.content[:tool_preview])\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"END\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "run_self_ask_verbose(q)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMEImNybxuzH",
        "outputId": "08d3ad0a-f3c3-48f9-d96c-9b82c09b5edb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "USER: AI Agent를 3문장으로 설명해줘.\n",
            "단, 2025~2026년 기준으로 최신 경향을 반영하기 위해 반드시 tavily_search_results_json을 최소 2번 사용하고,\n",
            "각 검색 결과를 근거로 Intermediate answer를 작성한 뒤 Final answer를 써.\n",
            "================================================================================\n",
            "\n",
            " TOOL CALLS:\n",
            " - {'name': 'tavily_search_results_json', 'args': {'query': 'AI Agent trends 2025 2026'}, 'id': 'call_miWfr6myVsDqfzuNOV8igawS', 'type': 'tool_call'}\n",
            "\n",
            " TOOL RESULT: tavily_search_results_json\n",
            "[{\"title\": \"Future of AI Agents: Top Trends in 2026 - Blue Prism\", \"url\": \"https://www.blueprism.com/resources/blog/future-ai-agents-trends/\", \"content\": \"That shift from promise to proof is the foundation of the 2026 agentic era. We’ve seen the rise of agentic AI in financial services, healthcare, manufacturing, etc., and leaders are waking up to a remarkably simple reality: AI workers aren’t coming, they’re already here. And they aren’t just assistants anymore. An intelligent agent is becoming more autonomous, where it manages complex workflows without needing constant human oversight.\\n\\nAg\n",
            "\n",
            " TOOL CALLS:\n",
            " - {'name': 'tavily_search_results_json', 'args': {'query': 'AI Agent developments 2025 2026'}, 'id': 'call_6VAhKFHvy1NHr5XkKUtPlIC9', 'type': 'tool_call'}\n",
            "\n",
            " TOOL RESULT: tavily_search_results_json\n",
            "[{\"title\": \"[PDF] 2026 Agentic Coding Trends Report | Anthropic\", \"url\": \"https://resources.anthropic.com/hubfs/2026%20Agentic%20Coding%20Trends%20Report.pdf?hsLang=en\", \"content\": \"8 Trend 3 Long-running agents build complete systems Early agents handled one-shot tasks that took a few minutes at most: fix this bug, write this function, generate this test. By late 2025, increasingly adept AI agents were producing full feature sets over the course of several hours. In 2026, agents will be able to work for days at a time, building entire applications and systems with minimal human intervention f\n",
            "\n",
            " AI MESSAGE:\n",
            " Intermediate answer 1:\n",
            "- By 2026, AI agents are expected to become more autonomous, managing complex workflows with minimal human oversight, and playing a crucial role in various industries like financial services and healthcare (Source: \"Future of AI Agents: Top Trends in 2026 - Blue Prism\").\n",
            "- AI agents will enhance productivity by taking over routine tasks, allowing employees to focus on strategic directions, and are predicted to become core components of business processes, offering hyperpersonalized services (Source: \"Google Just Predicted 5 AI Agent Trends for 2026. Here's What I'm ...\").\n",
            "\n",
            "Follow-up 2: AI Agent developments 2025 2026\n",
            "Intermediate answer 2:\n",
            "- In 2026, AI agents are expected to evolve from handling discrete tasks to working autonomously for extended periods, building entire applications and systems with minimal human intervention (Source: \"[PDF] 2026 Agentic Coding Trends Report | Anthropic\").\n",
            "- The AI agent landscape is becoming more dynamic, with companies like Anthropic leading the way in developing useful AI agent applications, focusing on offline learning and intelligent collaboration (Source: \"Current AI Agent Landscape: Jan 2026 | by Yashwanth Sai - Medium\").\n",
            "\n",
            "Final answer:\n",
            "By 2026, AI agents are anticipated to become more autonomous, managing complex workflows across industries with minimal human oversight. They will enhance productivity by taking over routine tasks, allowing employees to focus on strategic directions, and will become integral to business processes by offering hyperpersonalized services. Additionally, AI agents will evolve to handle long-term projects autonomously, building entire systems with minimal human intervention, and the landscape will be shaped by innovative companies focusing on intelligent collaboration and offline learning.\n",
            "\n",
            "================================================================================\n",
            "END\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. ReAct docstore\n",
        "\n",
        "*   LLM이 문서 저장소를 스스로 탐색\n",
        "*   필요한 문서를 조회(Lookup)하여 근거 기반 답변을 생성\n",
        "\n"
      ],
      "metadata": {
        "id": "BV3CJLc87yf1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ㄴ 4.1 Description"
      ],
      "metadata": {
        "id": "InnjMGqM-anr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ㄴ 4.1.1 Workflow"
      ],
      "metadata": {
        "id": "6YZ6Asa8-gQ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```javascript\n",
        "Client\n",
        "  ↓\n",
        "User Question 전송\n",
        "  ↓\n",
        "API / Backend (Agent Endpoint)\n",
        "  ↓\n",
        "LangGraph Runtime 시작\n",
        "  ↓\n",
        "LLM: 질문 해석 (문서 탐색 필요 판단)\n",
        "  ↓\n",
        "Docstore Search Tool 호출 (Search Action)\n",
        "  ↓\n",
        "관련 문서 후보 목록 반환 (Observation)\n",
        "  ↓\n",
        "LLM: 어떤 문서를 읽을지 Reasoning\n",
        "  ↓\n",
        "Docstore Lookup Tool 호출 (Lookup Action)\n",
        "  ↓\n",
        "선택된 문서 실제 내용 반환 (Observation)\n",
        "  ↓\n",
        "LLM이 문서 내용을 기반으로 재해석 / 근거 정리\n",
        "  ↓\n",
        "(필요 시 Search → Lookup 반복)\n",
        "  ↓\n",
        "충분한 근거 확보 후 Final Answer 생성\n",
        "  ↓\n",
        "API가 응답 반환\n",
        "  ↓\n",
        "Client가 사용자에게 표시\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "DCYLWB1f-mDf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ㄴ 4.1.2 Inner ReAct Loop"
      ],
      "metadata": {
        "id": "irEIUmY5-p7q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```javascript\n",
        "while (답변에 필요한 근거가 충분하지 않다):\n",
        "    Thought: 질문을 해결하려면 어떤 문서를 찾아야 하는가?\n",
        "    Action: Search(query)\n",
        "    Observation: 관련 문서 후보 목록 반환\n",
        "    Thought: 어떤 문서를 읽어야 하는가?\n",
        "    Action: Lookup(document_id)\n",
        "    Observation: 문서 내용 확보\n",
        "    Thought:이 정보로 답변 가능한가?\n",
        "            ├─ NO → 다른 문서 다시 Search\n",
        "            └─ YES → 반복 종료\n",
        "```"
      ],
      "metadata": {
        "id": "moO_SZRR-ubk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ㄴ 4.2 Code"
      ],
      "metadata": {
        "id": "Pjc24Na8-yza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "agent = create_react_agent(llm, tools)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUNy0ToZ-o0S",
        "outputId": "3a00f5d5-edb3-4a79-b682-140e232e5e6c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2357398768.py:3: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
            "  agent = create_react_agent(llm, tools)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q = \"\"\"\n",
        "규칙:\n",
        "- wikipedia 도구를 1회 호출해서 후보 제목을 찾는다.\n",
        "- 그 다음 docstore_lookup 도구를 반드시 1회 이상 호출한다.\n",
        "- docstore_lookup 결과를 근거로 최종 답변을 작성한다.\n",
        "\n",
        "질문: 밍크 선인장(Mammillaria)은 어떤 식물인지 설명해줘.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Scbnrykx-ecp"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "config = {\"configurable\": {\"thread_id\": \"docstore-01\"}}\n",
        "result = agent.invoke({\"messages\": [(\"user\", q)]},config=config)\n",
        "\n",
        "print(result[\"messages\"][-1].content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHLW_HGiBPLf",
        "outputId": "bcfaf8aa-a081-4afe-c3e4-cde3ae34d687"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "밍크 선인장(Mammillaria)은 선인장과(Cactaceae)에서 가장 큰 속 중 하나로, 현재 200여 종과 변종이 알려져 있습니다. 대부분의 밍크 선인장은 멕시코가 원산지이며, 일부는 미국 남서부, 카리브해, 콜롬비아, 과테말라, 온두라스, 베네수엘라에서도 발견됩니다. 이 속은 일반적으로 \"핀쿠션 선인장\"으로 불리며, 이는 이 속과 밀접하게 관련된 Escobaria와도 관련이 있습니다.\n",
            "\n",
            "밍크 선인장의 첫 번째 종은 1753년 칼 린네에 의해 Cactus mammillaris로 기술되었으며, 속명은 라틴어로 \"젖꼭지\"를 의미하는 mammilla에서 유래되었습니다. 이는 이 속의 독특한 특징 중 하나인 결절을 가리킵니다. 여러 종은 구형 선인장, 젖꼭지 선인장, 생일 케이크 선인장, 낚시바늘 선인장 또는 핀쿠션 선인장으로도 알려져 있습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ㄴ 4.3 Verbose"
      ],
      "metadata": {
        "id": "Lr3TMvzl-2Q-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage, ToolMessage\n",
        "\n",
        "def run_docstore_verbose(question: str, thread_id=\"docstore-debug\"):\n",
        "    cfg = {\"configurable\": {\"thread_id\": thread_id}}\n",
        "\n",
        "    print(\"\\n================ USER ================\")\n",
        "    print(question)\n",
        "\n",
        "    for step in agent.stream(\n",
        "        {\"messages\": [(\"user\", question)]},\n",
        "        config=cfg,\n",
        "        stream_mode=\"values\",   # 상태 변화 전부 받기\n",
        "    ):\n",
        "        msg = step[\"messages\"][-1]\n",
        "\n",
        "        # LLM이 생각한 내용 (Thought / Action 결정)\n",
        "        if isinstance(msg, AIMessage):\n",
        "            if msg.content:\n",
        "                print(\"\\n AI THOUGHT:\")\n",
        "                print(msg.content)\n",
        "\n",
        "            if getattr(msg, \"tool_calls\", None):\n",
        "                print(\"\\n TOOL CALL:\")\n",
        "                print(msg.tool_calls)\n",
        "\n",
        "        # Tool 실행 결과 (Observation)\n",
        "        elif isinstance(msg, ToolMessage):\n",
        "            print(\"\\n TOOL RESULT:\", msg.name)\n",
        "            print(msg.content[:1000])  # 너무 길어서 제한\n",
        "\n",
        "    print(\"\\n================ END ================\\n\")\n",
        "\n",
        "run_docstore_verbose(q)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5LEMXZFDYvI",
        "outputId": "6c370b60-c608-4e30-9ef3-21222f970b8b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================ USER ================\n",
            "\n",
            "규칙:\n",
            "- wikipedia 도구를 1회 호출해서 후보 제목을 찾는다.\n",
            "- 그 다음 docstore_lookup 도구를 반드시 1회 이상 호출한다.\n",
            "- docstore_lookup 결과를 근거로 최종 답변을 작성한다.\n",
            "\n",
            "질문: 밍크 선인장(Mammillaria)은 어떤 식물인지 설명해줘.\n",
            "\n",
            "\n",
            " TOOL CALL:\n",
            "[{'name': 'wikipedia', 'args': {'query': 'Mammillaria'}, 'id': 'call_ogxtJBav1SeJvLoBBHt6PIqA', 'type': 'tool_call'}]\n",
            "\n",
            " TOOL RESULT: wikipedia\n",
            "Page: Mammillaria\n",
            "Summary: Mammillaria is one of the largest genera in the cactus family (Cactaceae), with currently 200 known species and varieties recognized. Most of the mammillarias are native to Mexico, while some come from the Southwestern United States, the Caribbean, Colombia, Guatemala, Honduras and Venezuela. The common name \"pincushion cactus\" refers to this genus and the closely related Escobaria.\n",
            "The first species was described by Carl Linnaeus as Cactus mammillaris in 1753, deriving its name from the Latin mammilla, \"nipple\", referring to the tubercles that are among the distinctive features of the genus. Numerous species are commonly known as globe cactus, nipple cactus, birthday cake cactus, fishhook cactus or pincushion cactus (though such terms are also commonly used for related taxa, such as Escobaria or Ferocactus).\n",
            "\n",
            "Page: Mammillaria heyderi\n",
            "Summary: Mammillaria heyderi (commonly known as the Little Nipple cactus) is a species of pincushion cactus in the tribe Cact\n",
            "\n",
            " TOOL CALL:\n",
            "[{'name': 'docstore_lookup', 'args': {'title': 'Mammillaria'}, 'id': 'call_CZXvEAYHDqLeEYsmNZMv59H8', 'type': 'tool_call'}]\n",
            "\n",
            " TOOL RESULT: docstore_lookup\n",
            "Page: Mammillaria\n",
            "Summary: Mammillaria is one of the largest genera in the cactus family (Cactaceae), with currently 200 known species and varieties recognized. Most of the mammillarias are native to Mexico, while some come from the Southwestern United States, the Caribbean, Colombia, Guatemala, Honduras and Venezuela. The common name \"pincushion cactus\" refers to this genus and the closely related Escobaria.\n",
            "The first species was described by Carl Linnaeus as Cactus mammillaris in 1753, deriving its name from the Latin mammilla, \"nipple\", referring to the tubercles that are among the distinctive features of the genus. Numerous species are commonly known as globe cactus, nipple cactus, birthday cake cactus, fishhook cactus or pincushion cactus (though such terms are also commonly used for related taxa, such as Escobaria or Ferocactus).\n",
            "\n",
            " AI THOUGHT:\n",
            "밍크 선인장(Mammillaria)은 선인장과(Cactaceae)에서 가장 큰 속 중 하나로, 현재 200여 종과 변종이 알려져 있습니다. 대부분의 밍크 선인장은 멕시코가 원산지이며, 일부는 미국 남서부, 카리브해, 콜롬비아, 과테말라, 온두라스, 베네수엘라에서도 발견됩니다. 이 속은 일반적으로 \"핀쿠션 선인장\"으로 불리며, 이는 이 속과 밀접하게 관련된 Escobaria와도 관련이 있습니다.\n",
            "\n",
            "밍크 선인장의 첫 번째 종은 1753년 칼 린네에 의해 Cactus mammillaris로 기술되었으며, 속의 독특한 특징 중 하나인 결절을 가리키는 라틴어 mammilla, 즉 \"젖꼭지\"에서 그 이름이 유래되었습니다. 많은 종들이 구형 선인장, 젖꼭지 선인장, 생일 케이크 선인장, 낚시바늘 선인장 또는 핀쿠션 선인장으로 일반적으로 알려져 있습니다.\n",
            "\n",
            "================ END ================\n",
            "\n"
          ]
        }
      ]
    }
  ]
}